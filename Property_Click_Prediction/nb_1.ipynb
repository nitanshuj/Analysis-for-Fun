{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "# from jsonschema import validate, ValidationError\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns in dataframes\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "dataset_path = \"G:\\\\My Drive\\\\Study\\\\Projects - Data\\\\Data Projects - Analysis for Fun 1\\\\data\\\\datasets-property-click-prediction\"\n",
    "\n",
    "\n",
    "# Function for Checking NULL Counts and NULL Percentages\n",
    "# ======================================================\n",
    "def check_null(df):\n",
    "        \n",
    "    null_counts = pd.DataFrame(df.isnull().sum(), columns=['Null_Count'])\n",
    "    null_percentage = pd.DataFrame(round((df.isnull().sum()/len(df))*100, 2), columns=['Null_Percentage'])\n",
    "    \n",
    "    # Combining the two dataframes\n",
    "    null_info = pd.concat([null_counts, null_percentage], axis = 1)\n",
    "\n",
    "    # Filtering to show only rows where Null_Count > 0\n",
    "    null_info = null_info[null_info['Null_Count'] > 0]\n",
    "    \n",
    "    return null_info   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading & Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Pre-processing the `property_details dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data = pd.read_csv(os.path.join(dataset_path, \"property_data_set.csv\"))\n",
    "print(\"\\n\",property_data.shape)\n",
    "display(property_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing we notice that the date column is in string format. We need to convert it to datetime.\n",
    "property_data['activation_date'] = pd.to_datetime(property_data['activation_date'], format='mixed') # ormat='ISO8601\n",
    "property_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are missing values in 3 columns: 'total_floor', 'building_type', and 'pin_code'.\n",
    "# We will fill these missing values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change some more data types to optimize memory usage.\n",
    "# We use a way to handle missing values as well while converting data types - using 'coerce'.\n",
    "\n",
    "property_data['total_floor'] = pd.to_numeric(property_data['total_floor'], errors='coerce').astype('Int16')\n",
    "property_data['pin_code'] = property_data['pin_code'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data[['total_floor', 'building_type', 'pin_code']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data['building_type'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data['building_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will get back to the missing values.\n",
    "# Since the missing values are meagre.\n",
    "# Hence we fill categorical columns with mode values and numerical columns with median values.\n",
    "\n",
    "# Categorical Imputation\n",
    "property_data['building_type'] = property_data['building_type'].fillna(property_data['building_type'].mode()[0])\n",
    "property_data['pin_code'] = property_data['pin_code'].fillna(property_data['pin_code'].mode()[0])\n",
    "\n",
    "# Numerical Imputation\n",
    "property_data['total_floor'] = property_data['total_floor'].fillna(property_data['total_floor'].median())\n",
    "\n",
    "# Now let's check if there are any more missing values in the dataset.\n",
    "property_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We successfully cleaned the dataset.\n",
    "# We will explore more on this dataset later when we have merged all the datsets.\n",
    "\n",
    "#  We save the cleaned dataset to a new csv file.\n",
    "property_data.to_csv(os.path.join(dataset_path, \"property_data_processed.csv\"), index=False)\n",
    "\n",
    "# Now let's check the further datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Pre-Processing the `property_photos dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_photos = pd.read_csv(os.path.join(dataset_path, \"property_photos.tsv\"), sep='\\t')\n",
    "print(\"\\n\",property_photos.shape)\n",
    "display(property_photos.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_photos.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Our end goal is to get the count of photos for each property.\n",
    "# The photos are stored as a string in the 'photo_urls' column. They are json formatted strings.\n",
    "# But first, let's correct the corrupted json values in the 'photo_urls' column.\n",
    "# Once the json values are corrected, we can count the number of photos.\n",
    "# \n",
    "\n",
    "# What NULLs signify??\n",
    "# ------------------\n",
    "# A null value or \"nan\" values in 'photo_urls' column means that the property has no photos, so will contain a zero photo count.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_json(x):\n",
    "    # if value is null put count with 0 photos\n",
    "    if x is np.nan or x == 'NaN':\n",
    "        return 0\n",
    "    else :\n",
    "        # Replace corrupted values then convert to json and get count of photos\n",
    "        return len(json.loads( x.replace('\\\\' , '').replace('{title','{\"title').replace(']\"' , ']').replace('],\"', ']\",\"') ))\n",
    "    \n",
    "property_photos['photo_count'] = property_photos['photo_urls'].apply(correct_json)\n",
    "\n",
    "# drop the 'photo_urls' column\n",
    "property_photos.drop('photo_urls', axis=1, inplace=True)\n",
    "\n",
    "display(property_photos.head())\n",
    "\n",
    "# Saving the cleaned dataset to a new csv file.\n",
    "property_photos.to_csv(os.path.join(dataset_path, \"property_photos_processed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading & Pre-processing the `property_interactions dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the `property_interactions dataset`\n",
    "# -------------------------------------------\n",
    "property_interactions = pd.read_csv(os.path.join(dataset_path, \"property_interactions.csv\"))\n",
    "print(\"\\n\",property_interactions.shape)\n",
    "display(property_interactions.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this case we want to count the number of unique interactions for each property_id.\n",
    "# Also we would want to count interactions in the first 3 days, 5 days, and 7 days respectively.\n",
    "\n",
    "# Hence for each property_id, we will also need listing creation date \"activation_date\", which is not available in this dataset.\n",
    "# We use the property details dataset for this purpose.\n",
    "\n",
    "property_data = pd.read_csv(os.path.join(dataset_path, \"property_data_processed.csv\"))\n",
    "\n",
    "# We now merge the property_interactions dataset with the property_data dataset on property_id.\n",
    "# But from the property_data dataset, we get the 'activation_date' column.\n",
    "\n",
    "property_interactions = property_interactions.merge(property_data[['property_id', 'activation_date']], on='property_id')\n",
    "property_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_interactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we see that the 'request_date' and 'activation_date' are NOT in datetime format.\n",
    "# So, we convert these columns to datetime format.\n",
    "\n",
    "property_interactions['request_date'] = pd.to_datetime(property_interactions['request_date'], format='mixed')\n",
    "property_interactions['activation_date'] = pd.to_datetime(property_interactions['activation_date'], format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_interactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that the columns are now in datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_interactions['property_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I need to do is generate the total interactions - counting all the property listing clicks for each unique property_id\n",
    "# Also, I need to generate the total interactions occurred withing 3, 5, and 7 days respectively of the activation of the listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count interactions within a given number of days from activation\n",
    "def count_interactions_within_days(group, days):\n",
    "    end_date = group['activation_date'].iloc[0] + pd.Timedelta(days=days)\n",
    "    return group[(group['request_date'] >= group['activation_date'].iloc[0]) & (group['request_date'] < end_date)].shape[0]\n",
    "\n",
    "# Group by property_id and calculate the required metrics\n",
    "result = property_interactions.groupby('property_id').apply(lambda x: pd.Series({\n",
    "    'total_interactions': x.shape[0],\n",
    "    'count_3_days': count_interactions_within_days(x, 3),\n",
    "    'count_5_days': count_interactions_within_days(x, 5),\n",
    "    'count_7_days': count_interactions_within_days(x, 7)\n",
    "})).reset_index()\n",
    "\n",
    "display(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.shape)\n",
    "result.to_csv(os.path.join(dataset_path, \"property_interactions_processed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Reading the cleaned datasets\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(dataset_path, \"property_data_processed.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(dataset_path, \"property_photos_processed.csv\"))\n",
    "df3 = pd.read_csv(os.path.join(dataset_path, \"property_interactions_processed.csv\"))\n",
    "\n",
    "# Merging the property_data_processed and property_photos_processed datasets on property_id\n",
    "property_merged = df1.merge(df2, on='property_id').merge(df3, on='property_id',  how='left')\n",
    "\n",
    "print(property_merged.shape)\n",
    "display(property_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the merged dataset to a new csv file.\n",
    "property_merged.to_csv(os.path.join(dataset_path, \"merged_property_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the a complete and merged dataset.\n",
    "# So we will begin further processing and analyzing this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
